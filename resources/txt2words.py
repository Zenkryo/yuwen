#! /usr/bin/env python3
import os
import glob
import re
import jieba
import json
import re

def split_pinyin(pinyin):
    """
    å°†æ‹¼éŸ³å­—ç¬¦ä¸²åˆ†éš”ä¸ºå•ä¸ªéŸ³èŠ‚ã€‚
    æ¯ä¸ªéŸ³èŠ‚ç”±å£°æ¯ï¼ˆå¯é€‰ï¼‰+éŸµæ¯æ„æˆï¼ŒéŸµæ¯ä¸­æœ€å¤šæœ‰ä¸€ä¸ªå¸¦å£°è°ƒçš„å­—æ¯ã€‚
    æ”¯æŒå¸¦å£°è°ƒçš„æ‹¼éŸ³ï¼ˆå¦‚ lÃ³ngtÃ©nghÇ”yuÃ¨ â†’ ['lÃ³ng', 'tÃ©ng', 'hÇ”', 'yuÃ¨']ï¼‰ã€
    ç©ºæ ¼åˆ†éš”ï¼ˆå¦‚ nÇ hÇoï¼‰ã€æ’‡å·åˆ†éš”ï¼ˆå¦‚ xi'anï¼‰ã€‚
    """
    # æ£€æŸ¥è¾“å…¥æ˜¯å¦ä¸ºç©ºæˆ–éå­—ç¬¦ä¸²
    if not pinyin or not isinstance(pinyin, str):
        return []

    # è§„èŒƒåŒ–è¾“å…¥ï¼šæ›¿æ¢å•å¼•å·å’Œåˆ†éš”ç¬¦
    pinyin = pinyin.replace("â€™", " ").replace("â€¢", " ").strip()

    # ç¬¬ä¸€æ­¥ï¼šæŒ‰ç©ºæ ¼åˆ†éš”
    parts = re.split(r'[\s]+', pinyin)
    pinyin_list = []

    # åˆæ³•æ‹¼éŸ³å£°æ¯ï¼ˆåŒ…æ‹¬ç©ºå£°æ¯ï¼‰
    initials = (
        'b|p|m|f|d|t|n|l|g|k|h|j|q|x|zh|ch|sh|r|z|c|s|y|w|'
    ).split('|')

    # åˆæ³•æ‹¼éŸ³éŸµæ¯ï¼ˆæ¶µç›–å¸¸è§éŸµæ¯ï¼Œæ”¯æŒå¸¦å£°è°ƒå’Œæ— å£°è°ƒï¼‰
    finals = (
        'a|o|e|i|u|Ã¼|ai|ei|ui|ao|ou|iu|ie|ue|er|an|en|in|ang|eng|ing|ong|un|Ã¼n|'
        'ia|ian|iang|iong|iao|'
        'ua|uai|uan|uang|uo|'
    ).split('|')

    # æŒ‰é•¿åˆ°çŸ­æ’åº
    finals.sort(key=len, reverse=True)

    # å¸¦å£°è°ƒçš„å…ƒéŸ³
    tones = r'[ÄÃ¡ÇÃ Ä“Ã©Ä›Ã¨Ä«Ã­ÇÃ¬ÅÃ³Ç’Ã²Å«ÃºÇ”Ã¹Ç–Ç˜ÇšÇœ]'

    # æ„å»ºæ­£åˆ™è¡¨è¾¾å¼ï¼š
    # (å£°æ¯)? (æ— å£°è°ƒéŸµæ¯ | å¸¦ä¸€ä¸ªå£°è°ƒçš„éŸµæ¯)
    # æ— å£°è°ƒéŸµæ¯ï¼šç›´æ¥åŒ¹é… finals
    # å¸¦å£°è°ƒéŸµæ¯ï¼šæ›¿æ¢éŸµæ¯ä¸­çš„å…ƒéŸ³ä¸ºå¸¦å£°è°ƒå…ƒéŸ³
    tone_finals = []
    for final in finals:
        if 'iu' == final:
            tone_finals.append('i[uÅ«ÃºÇ”Ã¹]')
        elif 'ui' == final:
            tone_finals.append('u[iÄ«Ã­ÇÃ¬]')
        elif "a" in final:
            tone_finals.append(final.replace('a', '[aÄÃ¡ÇÃ ]'))
        elif "o" in final:
            tone_finals.append(final.replace('o', '[oÅÃ³Ç’Ã²]'))
        elif "e" in final:
            tone_finals.append(final.replace('e', '[eÄ“Ã©Ä›Ã¨]'))
        elif final.startswith('i'):
            tone_finals.append(f'[iÄ«Ã­ÇÃ¬]{final[1:]}')
        elif final.startswith('u'):
            tone_finals.append(f'[uÅ«ÃºÇ”Ã¹]{final[1:]}')
        elif final.startswith('Ã¼'):
            tone_finals.append(f'[Ã¼Ç–Ç˜ÇšÇœ]{final[1:]}')
        elif final.startswith('v'):
            tone_finals.append(f'[vÇ–Ç˜ÇšÇœ]{final[1:]}')
        else:
            tone_finals.append(final)  # æ— å…ƒéŸ³çš„éŸµæ¯ä¿æŒä¸å˜

    # å¯é€‰çš„å£°æ¯éƒ¨åˆ†+éŸµæ¯éƒ¨åˆ†
    syllable_pattern = rf"({'|'.join(initials)})?({'|'.join(tone_finals)})"
    for part in parts:
        # å¤„ç†æ’‡å·åˆ†éš”çš„éƒ¨åˆ†ï¼ˆä¾‹å¦‚ xi'an â†’ xi + anï¼‰
        sub_parts = part.split('\'')
        for sub_part in sub_parts:
            if not sub_part:
                continue
            # æŸ¥æ‰¾å­éƒ¨åˆ†ä¸­çš„æ‰€æœ‰éŸ³èŠ‚
            matches = re.finditer(syllable_pattern, sub_part, re.IGNORECASE)
            syllables = [match.group(0) for match in matches if match.group(0)]  # åªä¿ç•™éç©ºåŒ¹é…
            # éªŒè¯æ˜¯å¦å®Œå…¨åŒ¹é…æ•´ä¸ªå­éƒ¨åˆ†
            if syllables and ''.join(syllables) == sub_part:
                pinyin_list.extend(syllables)
            else:
                pinyin_list.append(sub_part)
    return pinyin_list

def process_special_chars(text):
    """
    å°†æ–‡æœ¬ä¸­çš„å…¨è§’å­—ç¬¦ï¼ˆå­—æ¯ã€æ•°å­—ï¼‰è½¬æ¢ä¸ºåŠè§’å­—ç¬¦ã€‚
    åŒæ—¶å°†æ‹‰ä¸æ–‡å°å†™è‰ä¹¦å­—æ¯å’Œå…¶ä»–ç‰¹æ®Šæ‹‰ä¸å­—æ¯è½¬æ¢ä¸ºå¯¹åº”çš„ASCIIå­—æ¯ã€‚
    """
    # ç¬¬ä¸€æ­¥ï¼šè½¬æ¢å…¨è§’å­—ç¬¦
    trans_dict1 = str.maketrans(
        'ï½ï½‚ï½ƒï½„ï½…ï½†ï½‡ï½ˆï½‰ï½Šï½‹ï½Œï½ï½ï½ï½ï½‘ï½’ï½“ï½”ï½•ï½–ï½—ï½˜ï½™ï½šï¼¡ï¼¢ï¼£ï¼¤ï¼¥ï¼¦ï¼§ï¼¨ï¼©ï¼ªï¼«ï¼¬ï¼­ï¼®ï¼¯ï¼°ï¼±ï¼²ï¼³ï¼´ï¼µï¼¶ï¼·ï¼¸ï¼¹ï¼ºï¼ï¼‘ï¼’ï¼“ï¼”ï¼•ï¼–ï¼—ï¼˜ï¼™',
        'abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789'
    )
    
    # ç¬¬äºŒæ­¥ï¼šè½¬æ¢æ‹‰ä¸æ–‡å°å†™è‰ä¹¦å­—æ¯
    trans_dict2 = str.maketrans(
        'ğ“ªğ“«ğ“¬ğ“­ğ“®ğ“¯ğ“°ğ“±ğ“²ğ“³ğ“´ğ“µğ“¶ğ“·ğ“¸ğ“¹ğ“ºğ“»ğ“¼ğ“½ğ“¾ğ“¿ğ”€ğ”ğ”‚ğ”ƒ',
        'abcdefghijklmnopqrstuvwxyz'
    )
    
    # ç¬¬ä¸‰æ­¥ï¼šè½¬æ¢å…¶ä»–ç‰¹æ®Šæ‹‰ä¸å­—æ¯
    special_chars = 'É¡É¢Ê›É•Ê—É–É—É˜É™ÉšÉ›ÉœÉÉÉŸÉ É¡É¢É£É¤É¥É¦É§É¨É©ÉªÉ«É¬É­É®É¯É°É±É²É³É´ÉµÉ¶É·É¸É¹ÉºÉ»É¼É½É¾É¿Ê€ÊÊ‚ÊƒÊ„Ê…Ê†Ê‡ÊˆÊ‰ÊŠÊ‹ÊŒÊÊÊÊÊ‘Ê’Ê“Ê”Ê•Ê–Ê—Ê˜Ê™ÊšÊ›ÊœÊÊÊŸÊ Ê¡Ê¢Ê£Ê¤Ê¥Ê¦Ê§Ê¨Ê©ÊªÊ«'
    normal_chars = 'gggccdddeeeeeeefggghhiiilllllmmmmnnnooopprrrrrrsssttuuvvwwyyzzzqhhkkllqqqzzzzzzzzzzzzzzzzzz'
    
    trans_dict3 = str.maketrans(special_chars, normal_chars)
    
    # ç¬¬å››æ­¥ï¼šè½¬æ¢ä¸Šæ ‡æ•°å­—
    trans_dict4 = str.maketrans(
        'â°Â¹Â²Â³â´âµâ¶â·â¸â¹',
        '0123456789'
    )
    
    # æ‰§è¡Œè½¬æ¢
    text = text.translate(trans_dict1)
    text = text.translate(trans_dict2)
    text = text.translate(trans_dict3)
    text = text.translate(trans_dict4)
    
    # å…¨è§’ç©ºæ ¼ (U+3000) è½¬æ¢ä¸ºåŠè§’ç©ºæ ¼ (U+0020)
    text = text.replace('ã€€', ' ')
    
    return text

# è§£æç°ä»£æ±‰è¯­è¯å…¸, å¾—åˆ°æ¯ä¸ªè¯ï¼Œä»¥åŠå¯¹åº”çš„æ‹¼éŸ³å’Œé‡Šä¹‰
def parse_cidian():
    # è¯»å–è¯å…¸æ–‡ä»¶
    with open('XDHYCD7th.txt', 'r', encoding='utf-8') as f:
        content = f.read()
    content = process_special_chars(content)
    # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼åŒ¹é…è¯æ¡
    # è¯æ¡æ ¼å¼ä¸ºï¼šã€è¯ã€‘æ‹¼éŸ³ é‡Šä¹‰
    pattern = r'ã€(.*?)ã€‘\s*\d*\s*([a-zA-ZÄÃ¡ÇÃ Ä“Ã©Ä›Ã¨Ä«Ã­ÇÃ¬ÅÃ³Ç’Ã²Å«ÃºÇ”Ã¹Ç–Ç˜ÇšÇœâ€¢â€™]+)\s*(.*)'
    entries = re.findall(pattern, content)
    
    # å­˜å‚¨è¯æ¡ä¿¡æ¯
    dictionary = {}
    for word, pinyin,defs in entries:
        if len(word) > 1:
            if word == "é¾™è…¾è™è·ƒ":
                print(pinyin)
            pinyin_list = split_pinyin(pinyin)
            if len(word) != len(pinyin_list):
                print(word, pinyin, pinyin_list, defs)
            dictionary[word] = {'pinyin': pinyin_list, 'explanation': defs}
    
    return dictionary

# è§£ææˆè¯­è¯å…¸, å¾—åˆ°æ¯ä¸ªæˆè¯­ï¼Œä»¥åŠå¯¹åº”çš„æ‹¼éŸ³ï¼Œè§£é‡Šï¼Œå‡ºå¤„ï¼Œä¾‹å¥
def parse_chengyu():
    with open('idiom.json', 'r', encoding='utf-8') as f:
        idiom_dict = json.load(f)
    idioms = {}
    for idiom in idiom_dict:
        idioms[idiom['word']] = idiom
        pinyin_list = split_pinyin(idiom['pinyin'])
        idioms[idiom['word']]['pinyin'] = pinyin_list
    return idioms

# è§£æè¯¾æœ¬ï¼Œå¾—åˆ°è¯¾æœ¬ä¸­æ‰€æœ‰çš„è¯è¯­
def parse_keben():
    # è·å–å½“å‰ç›®å½•ä¸‹æ‰€æœ‰çš„txtæ–‡ä»¶
    txt_files = glob.glob('è¯­æ–‡*.txt')

    # ç”¨äºå­˜å‚¨æ‰€æœ‰ä¸é‡å¤çš„è¯è¯­
    all_words = set()

    # éå†æ¯ä¸ªtxtæ–‡ä»¶
    for txt_file in txt_files:
        print(f'Processing {txt_file}...')
        
        # è¯»å–æ–‡ä»¶å†…å®¹
        with open(txt_file, 'r', encoding='utf-8') as f:
            text = f.read()
        
        # åªä¿ç•™æ±‰å­—å’Œæ ‡ç‚¹ç¬¦å·
        text = re.sub(r'[^\u4e00-\u9fff\s]', '', text)
        
        # ä½¿ç”¨jiebaè¿›è¡Œåˆ†è¯
        words = jieba.cut(text)
        
        # å°†è¯è¯­æ·»åŠ åˆ°é›†åˆä¸­
        all_words.update(words)
    return all_words

def parse_chengyu300():
    with open('æˆè¯­300.txt', 'r', encoding='utf-8') as f:
        content = f.read()
    words = set()
    for line in content.split('\n'):
        for word in line.split(' '):
            if word not in words:
                words.add(word)
    return words



if __name__ == '__main__':
    # è§£æç°ä»£æ±‰è¯­è¯å…¸
    cidian = parse_cidian()

    # è§£ææˆè¯­è¯å…¸
    chengyu = parse_chengyu()

    # è§£æè¯¾æœ¬
    words = parse_keben()

    # è§£ææˆè¯­3000
    chengyu300 = parse_chengyu300()

    all_words = words | chengyu300

    # for word in sorted(all_words):
    #     if word in cidian:
    #         print(word," ", cidian[word]['pinyin'], " ", cidian[word]['explanation'])
    #     elif word in chengyu:
    #         print(word," ", chengyu[word]['pinyin'], " ", chengyu[word]['explanation'])
