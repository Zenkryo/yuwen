#! /usr/bin/env python3
import os
import glob
import re
import jieba
import json

def split_pinyin(pinyin):
    # å…ˆæŒ‰ç©ºæ ¼æˆ–åˆ†éš”ç¬¦ï¼ˆ'â€¢', ''', "'"ï¼‰åˆ†å‰²æˆå¤šä¸ªéƒ¨åˆ†
    parts = re.split(r'[\sâ€¢â€™]+', pinyin)
    
    # å¯¹æ¯ä¸ªéƒ¨åˆ†å•ç‹¬è¿›è¡Œæ‹¼éŸ³åˆ†å‰²
    pinyin_list = []
    for part in parts:
        # åŒ¹é…å¸¦å£°è°ƒçš„å­—æ¯æˆ–æ™®é€šéŸ³èŠ‚ï¼Œæ­£ç¡®å¤„ç† qu, ju, xu, yu ç­‰æƒ…å†µ
        sub_matches = re.finditer(
            r'(?:[b-df-hj-np-tv-z]|qu|ju|xu|yu)[a-z]*[ÄÃ¡ÇÃ Ä“Ã©Ä›Ã¨Ä«Ã­ÇÃ¬ÅÃ³Ç’Ã²Å«ÃºÇ”Ã¹Ç–Ç˜ÇšÇœ]?[a-z]*',
            part,
            re.IGNORECASE
        )
        pinyin_list.extend([match.group(0) for match in sub_matches])
    
    return pinyin_list

def process_special_chars(text):
    """
    å°†æ–‡æœ¬ä¸­çš„å…¨è§’å­—ç¬¦ï¼ˆå­—æ¯ã€æ•°å­—ï¼‰è½¬æ¢ä¸ºåŠè§’å­—ç¬¦ã€‚
    åŒæ—¶å°†æ‹‰ä¸æ–‡å°å†™è‰ä¹¦å­—æ¯å’Œå…¶ä»–ç‰¹æ®Šæ‹‰ä¸å­—æ¯è½¬æ¢ä¸ºå¯¹åº”çš„ASCIIå­—æ¯ã€‚
    """
    # ç¬¬ä¸€æ­¥ï¼šè½¬æ¢å…¨è§’å­—ç¬¦
    trans_dict1 = str.maketrans(
        'ï½ï½‚ï½ƒï½„ï½…ï½†ï½‡ï½ˆï½‰ï½Šï½‹ï½Œï½ï½ï½ï½ï½‘ï½’ï½“ï½”ï½•ï½–ï½—ï½˜ï½™ï½šï¼¡ï¼¢ï¼£ï¼¤ï¼¥ï¼¦ï¼§ï¼¨ï¼©ï¼ªï¼«ï¼¬ï¼­ï¼®ï¼¯ï¼°ï¼±ï¼²ï¼³ï¼´ï¼µï¼¶ï¼·ï¼¸ï¼¹ï¼ºï¼ï¼‘ï¼’ï¼“ï¼”ï¼•ï¼–ï¼—ï¼˜ï¼™',
        'abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789'
    )
    
    # ç¬¬äºŒæ­¥ï¼šè½¬æ¢æ‹‰ä¸æ–‡å°å†™è‰ä¹¦å­—æ¯
    trans_dict2 = str.maketrans(
        'ğ“ªğ“«ğ“¬ğ“­ğ“®ğ“¯ğ“°ğ“±ğ“²ğ“³ğ“´ğ“µğ“¶ğ“·ğ“¸ğ“¹ğ“ºğ“»ğ“¼ğ“½ğ“¾ğ“¿ğ”€ğ”ğ”‚ğ”ƒ',
        'abcdefghijklmnopqrstuvwxyz'
    )
    
    # ç¬¬ä¸‰æ­¥ï¼šè½¬æ¢å…¶ä»–ç‰¹æ®Šæ‹‰ä¸å­—æ¯
    special_chars = 'É¡É¢Ê›É•Ê—É–É—É˜É™ÉšÉ›ÉœÉÉÉŸÉ É¡É¢É£É¤É¥É¦É§É¨É©ÉªÉ«É¬É­É®É¯É°É±É²É³É´ÉµÉ¶É·É¸É¹ÉºÉ»É¼É½É¾É¿Ê€ÊÊ‚ÊƒÊ„Ê…Ê†Ê‡ÊˆÊ‰ÊŠÊ‹ÊŒÊÊÊÊÊ‘Ê’Ê“Ê”Ê•Ê–Ê—Ê˜Ê™ÊšÊ›ÊœÊÊÊŸÊ Ê¡Ê¢Ê£Ê¤Ê¥Ê¦Ê§Ê¨Ê©ÊªÊ«'
    normal_chars = 'gggccdddeeeeeeefggghhiiilllllmmmmnnnooopprrrrrrsssttuuvvwwyyzzzqhhkkllqqqzzzzzzzzzzzzzzzzzz'
    
    trans_dict3 = str.maketrans(special_chars, normal_chars)
    
    # ç¬¬å››æ­¥ï¼šè½¬æ¢ä¸Šæ ‡æ•°å­—
    trans_dict4 = str.maketrans(
        'â°Â¹Â²Â³â´âµâ¶â·â¸â¹',
        '0123456789'
    )
    
    # æ‰§è¡Œè½¬æ¢
    text = text.translate(trans_dict1)
    text = text.translate(trans_dict2)
    text = text.translate(trans_dict3)
    text = text.translate(trans_dict4)
    
    # å…¨è§’ç©ºæ ¼ (U+3000) è½¬æ¢ä¸ºåŠè§’ç©ºæ ¼ (U+0020)
    text = text.replace('ã€€', ' ')
    
    return text

# è§£æç°ä»£æ±‰è¯­è¯å…¸, å¾—åˆ°æ¯ä¸ªè¯ï¼Œä»¥åŠå¯¹åº”çš„æ‹¼éŸ³å’Œé‡Šä¹‰
def parse_cidian():
    # è¯»å–è¯å…¸æ–‡ä»¶
    with open('XDHYCD7th.txt', 'r', encoding='utf-8') as f:
        content = f.read()
    content = process_special_chars(content)
    # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼åŒ¹é…è¯æ¡
    # è¯æ¡æ ¼å¼ä¸ºï¼šã€è¯ã€‘æ‹¼éŸ³ é‡Šä¹‰
    pattern = r'ã€(.*?)ã€‘\s*\d*\s*([a-zA-ZÄÃ¡ÇÃ Ä“Ã©Ä›Ã¨Ä«Ã­ÇÃ¬ÅÃ³Ç’Ã²Å«ÃºÇ”Ã¹Ç–Ç˜ÇšÇœâ€¢'']+)\s*(.*)'
    entries = re.findall(pattern, content)
    
    # å­˜å‚¨è¯æ¡ä¿¡æ¯
    dictionary = {}
    for word, pinyin,defs in entries:
        if len(word) > 1:
            if word == "é¾™è…¾è™è·ƒ":
                print(pinyin)
            pinyin_list = split_pinyin(pinyin)
            dictionary[word] = {'pinyin': pinyin_list, 'explanation': defs}
    
    return dictionary

# è§£ææˆè¯­è¯å…¸, å¾—åˆ°æ¯ä¸ªæˆè¯­ï¼Œä»¥åŠå¯¹åº”çš„æ‹¼éŸ³ï¼Œè§£é‡Šï¼Œå‡ºå¤„ï¼Œä¾‹å¥
def parse_chengyu():
    with open('idiom.json', 'r', encoding='utf-8') as f:
        idiom_dict = json.load(f)
    idioms = {}
    for idiom in idiom_dict:
        idioms[idiom['word']] = idiom
        pinyin_list = split_pinyin(idiom['pinyin'])
        idioms[idiom['word']]['pinyin'] = pinyin_list
    return idioms

# è§£æè¯¾æœ¬ï¼Œå¾—åˆ°è¯¾æœ¬ä¸­æ‰€æœ‰çš„è¯è¯­
def parse_keben():
    # è·å–å½“å‰ç›®å½•ä¸‹æ‰€æœ‰çš„txtæ–‡ä»¶
    txt_files = glob.glob('è¯­æ–‡*.txt')

    # ç”¨äºå­˜å‚¨æ‰€æœ‰ä¸é‡å¤çš„è¯è¯­
    all_words = set()

    # éå†æ¯ä¸ªtxtæ–‡ä»¶
    for txt_file in txt_files:
        print(f'Processing {txt_file}...')
        
        # è¯»å–æ–‡ä»¶å†…å®¹
        with open(txt_file, 'r', encoding='utf-8') as f:
            text = f.read()
        
        # åªä¿ç•™æ±‰å­—å’Œæ ‡ç‚¹ç¬¦å·
        text = re.sub(r'[^\u4e00-\u9fff\s]', '', text)
        
        # ä½¿ç”¨jiebaè¿›è¡Œåˆ†è¯
        words = jieba.cut(text)
        
        # å°†è¯è¯­æ·»åŠ åˆ°é›†åˆä¸­
        all_words.update(words)
    return all_words

def parse_chengyu300():
    with open('æˆè¯­300.txt', 'r', encoding='utf-8') as f:
        content = f.read()
    words = set()
    for line in content.split('\n'):
        for word in line.split(' '):
            if word not in words:
                words.add(word)
    return words



if __name__ == '__main__':
    # è§£æç°ä»£æ±‰è¯­è¯å…¸
    cidian = parse_cidian()

    # è§£ææˆè¯­è¯å…¸
    chengyu = parse_chengyu()

    # è§£æè¯¾æœ¬
    words = parse_keben()

    # è§£ææˆè¯­3000
    chengyu300 = parse_chengyu300()

    all_words = words | chengyu300

    for word in sorted(all_words):
        if word in cidian:
            print(word," ", cidian[word]['pinyin'], " ", cidian[word]['explanation'])
        elif word in chengyu:
            print(word," ", chengyu[word]['pinyin'], " ", chengyu[word]['explanation'])
